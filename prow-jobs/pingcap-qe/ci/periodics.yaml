# struct ref: https://pkg.go.dev/sigs.k8s.io/prow/pkg/config#Periodic
periodics:
  - name: periodic-crawl-ci-run-data
    decorate: true # need add this.
    cron: "0 * * * *" # @hourly
    hidden: true
    spec:
      activeDeadlineSeconds: 300
      containers:
        - name: main
          image: denoland/deno:2.2.3
          command:
            - deno
            - run
            - --allow-net
            - https://github.com/PingCAP-QE/ee-apps/raw/main/insight/crawlers/ci/prow-jobs.ts
          args: [--dsn=$(DSN)]
          env:
            - name: DSN
              valueFrom:
                secretKeyRef:
                  key: DSN
                  name: job-insight
          resources:
            requests:
              memory: 1Gi
              cpu: 200m
  - name: periodic-crawl-commits
    cron: "0 * * * *" # Run @hourly
    hidden: true
    decorate: true
    decoration_config:
      timeout: 12h
      oauth_token_secret:
        name: github-token
        key: token
    extra_refs:
      - org: PingCAP-QE
        repo: gitinsight
        path_alias: app
        base_ref: main
        clone_depth: 1
        workdir: true
      - org: PingCAP-QE
        repo: gitinsight-data
        base_ref: main
        path_alias: data
        workdir: false
    max_concurrency: 1 # Only allow one instance of this job to run at a time
    spec:
      volumes:
        - name: insight-config
          secret:
            secretName: job-insight
        - name: shared-data
          emptyDir: {}
      initContainers:
        - name: setup
          image: ubuntu:latest
          command:
            - /bin/bash
            - -ceo
            - pipefail
          args:
            - |
              # Prepare directory structure
              mkdir -p ../data/repos
              mkdir -p ../data/.ci/config
              cp "$IDS_CSV_FILE" ../data/.ci/config/pingcap-ids.csv

              # Copy to shared volume for status tracking
              mkdir -p /shared/status
          env:
            - name: IDS_CSV_FILE
              value: /tmp/ids.csv
          volumeMounts: &shared-volume-mount
            - name: shared-data
              mountPath: /shared
            - name: insight-config
              mountPath: /tmp/ids.csv
              subPath: pingcap-ids.csv
      containers:
        - &crawler-base
          name: crawl-pingcap
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_ORG
              value: pingcap
          image: ubuntu:latest
          command:
            - /bin/bash
            - -ceo
            - pipefail
          args:
            - |
              # Setup environment
              apt-get update && apt-get install -y git curl wget jq

              # Set up pixi
              curl -fsSL https://pixi.sh/install.sh | bash
              export PATH="$HOME/.pixi/bin:$PATH"
              pixi install

              # Crawl commits for the org
              pixi run ../data/.ci/crawl_org_commits.sh ${CRAWL_ORG} ../data ../data/.ci/config 132

              # Mark as finished
              touch /shared/status/${CRAWL_ORG}.done
          volumeMounts:
            - name: shared-data
              mountPath: /shared
          resources:
            requests:
              cpu: "2"
              memory: 2Gi
        - <<: *crawler-base
          name: crawl-tikv
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_ORG
              value: tikv
        - <<: *crawler-base
          name: crawl-tidbcloud
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_ORG
              value: tidbcloud
        - <<: *crawler-base
          name: crawl-pingcap-qe
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_ORG
              value: PingCAP-QE
        - name: publish
          image: ubuntu:latest
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_ORGS
              value: pingcap tikv tidbcloud PingCAP-QE
          command:
            - /bin/bash
            - -ceo
            - pipefail
          args:
            - |
              # Wait for all crawling to complete
              while true; do
                all_done=true
                for org in $CRAWL_ORGS; do
                  if [[ ! -f "/shared/status/${org}.done" ]]; then
                    all_done=false
                    break
                  fi
                done

                if [[ "$all_done" == "true" ]]; then
                  echo "All crawling jobs completed."
                  break
                else
                  echo "Waiting for all crawling to complete..."
                  sleep 30
                fi
              done

              # Setup environment
              apt-get update && apt-get install -y git curl wget jq

              # Set up pixi
              curl -fsSL https://pixi.sh/install.sh | bash
              export PATH="$HOME/.pixi/bin:$PATH"
              pixi install

              # Cleanup
              rm -f ../data/.ci/config/pingcap-ids.csv

              # Publish data
              pushd ../data
                # Set git account
                git config user.name "ti-chi-bot[bot]"
                git config user.email "108142056+ti-chi-bot[bot]@users.noreply.github.com"
                git remote add origin https://${GITHUB_TOKEN}@github.com/PingCAP-QE/gitinsight-data.git

                # Checkout to a new branch with timestamp
                head_branch="auto/update-data-commits-$(date +%Y%m%d%H%M%S)"
                pr_title="Update commits data at $(date +%Y-%m-%d)"
                pr_desc="It is made by periodic jobs for data update. [skip ci]"

                git checkout -b $head_branch
                git add github

                git commit -m "${pr_title} [skip ci]"
                git push --set-upstream origin "$head_branch"

                # create a pull request and merge it
                which gh || pixi global install gh
                gh pr create --title "$pr_title" --body "$pr_desc" --head $head_branch --base main
                gh pr merge --squash --delete-branch || echo "Some error happened when merge the pr"
              popd
          volumeMounts:
            - name: shared-data
              mountPath: /shared
          resources:
            requests:
              cpu: "100m"
              memory: 128Mi

  - name: periodic-crawl-pulls-issues
    cron: "30 * * * *" # @hourly
    hidden: true
    decorate: true
    decoration_config:
      timeout: 23h30m
      oauth_token_secret:
        name: github-token
        key: token
    extra_refs:
      - org: PingCAP-QE
        repo: gitinsight
        path_alias: app
        base_ref: main
        clone_depth: 1
        workdir: true
      - org: PingCAP-QE
        repo: gitinsight-data
        base_ref: main
        path_alias: data
        workdir: false
    max_concurrency: 1 # Only allow one instance of this job to run at a time
    spec:
      volumes:
        - name: insight-config
          secret:
            secretName: job-insight
        - name: shared-data
          emptyDir: {}
      containers:
        - &issues-pulls-crawler-base
          name: crawl-pingcap-tidb
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_REPO
              value: pingcap/tidb
            - name: START_DATE
              value: "2015-01-01"
          image: ubuntu:latest
          command:
            - /bin/bash
            - -ceo
            - pipefail
          args:
            - |
              # Setup environment
              apt-get update && apt-get install -y git curl wget jq

              # Set up pixi
              curl -fsSL https://pixi.sh/install.sh | bash
              export PATH="$HOME/.pixi/bin:$PATH"
              pixi install

              # Crawl pulls and issues for the repo
              timeout 82800 pixi run ../data/.ci/crawl_repo_pulls_issues.sh ${CRAWL_REPO} ../data ${START_DATE} || echo "⌚️ timeout in 23 hour."

              # Mark as finished
              mkdir -p $(dirname /shared/status/${CRAWL_REPO}.done)
              touch /shared/status/${CRAWL_REPO}.done
          volumeMounts:
            - name: shared-data
              mountPath: /shared
          resources:
            requests:
              cpu: "1"
              memory: 1Gi
            limits:
              cpu: "4"
              memory: 4Gi
        - <<: *issues-pulls-crawler-base
          name: crawl-pingcap-tiflash
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_REPO
              value: pingcap/tiflash
            - name: START_DATE
              value: "2015-01-01"
        - <<: *issues-pulls-crawler-base
          name: crawl-pingcap-tiflow
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_REPO
              value: pingcap/tiflow
            - name: START_DATE
              value: "2015-01-01"
        - <<: *issues-pulls-crawler-base
          name: crawl-pingcap-ticdc
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_REPO
              value: pingcap/ticdc
            - name: START_DATE
              value: "2015-01-01"
        - <<: *issues-pulls-crawler-base
          name: crawl-tikv-pd
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_REPO
              value: tikv/pd
            - name: START_DATE
              value: "2015-01-01"
        - <<: *issues-pulls-crawler-base
          name: crawl-tikv-tikv
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_REPO
              value: tikv/tikv
            - name: START_DATE
              value: "2015-01-01"
        - name: publish
          image: ubuntu:latest
          env:
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CRAWL_REPOS
              value: pingcap/tidb pingcap/tiflash pingcap/tiflow pingcap/ticdc tikv/pd tikv/tikv
          command:
            - /bin/bash
            - -ceo
            - pipefail
          args:
            - |
              # Wait for all crawling to complete
              while true; do
                all_done=true
                for repo in $CRAWL_REPOS; do
                  if [[ ! -f "/shared/status/${repo}.done" ]]; then
                    all_done=false
                    break
                  fi
                done

                if [[ "$all_done" == "true" ]]; then
                  echo "All crawling jobs completed."
                  break
                else
                  echo "Waiting for all crawling to complete..."
                  sleep 30
                fi
              done

              # Setup environment
              apt-get update && apt-get install -y git curl wget jq

              # Set up pixi
              curl -fsSL https://pixi.sh/install.sh | bash
              export PATH="$HOME/.pixi/bin:$PATH"
              pixi install

              # Publish data
              pushd ../data
                # Set git account
                git config user.name "ti-chi-bot[bot]"
                git config user.email "108142056+ti-chi-bot[bot]@users.noreply.github.com"
                git remote add origin https://${GITHUB_TOKEN}@github.com/PingCAP-QE/gitinsight-data.git

                # Checkout to a new branch with timestamp
                head_branch="auto/update-data-issues-pulls-$(date +%Y%m%d%H%M%S)"
                pr_title="Update issues and pulls data at $(date +%Y-%m-%d)"
                pr_desc="It is made by periodic jobs for data update. [skip ci]"

                git checkout -b $head_branch
                git add github

                git commit -m "${pr_title} [skip ci]"
                git push --set-upstream origin $head_branch

                # create a pull request and merge it
                which gh || pixi global install gh
                gh pr create --title "$pr_title" --body "$pr_desc" --head $head_branch --base main
                gh pr merge --squash --delete-branch || echo "Some error happened when merge the pr"
              popd
          volumeMounts:
            - name: shared-data
              mountPath: /shared
          resources:
            requests:
              cpu: "100m"
              memory: 128Mi
