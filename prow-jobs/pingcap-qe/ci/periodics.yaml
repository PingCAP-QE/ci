# struct ref: https://pkg.go.dev/sigs.k8s.io/prow/pkg/config#Periodic
periodics:
  - name: periodic-crawl-ci-run-data
    decorate: true # need add this.
    cron: "0 * * * *" # @hourly
    hidden: true
    spec:
      activeDeadlineSeconds: 300
      containers:
        - name: main
          image: denoland/deno:2.2.3
          command:
            - deno
            - run
            - --allow-net
            - https://github.com/PingCAP-QE/ee-apps/raw/main/insight/crawlers/ci/prow-jobs.ts
          args: [--dsn=$(DSN)]
          env:
            - name: DSN
              valueFrom:
                secretKeyRef:
                  key: DSN
                  name: job-insight
          resources:
            requests:
              memory: 1Gi
              cpu: 200m
  - name: periodic-crawl-commits
    cron: "0 * * * *" # Run @hourly
    hidden: true
    decorate: true
    decoration_config:
      timeout: 12h
      oauth_token_secret:
        name: github-token
        key: token
    extra_refs:
      - org: PingCAP-QE
        repo: gitinsight
        path_alias: app
        base_ref: main
        clone_depth: 1
        workdir: true
      - org: PingCAP-QE
        repo: gitinsight-data
        base_ref: main
        path_alias: data
        workdir: false
    max_concurrency: 1 # Only allow one instance of this job to run at a time
    spec:
      containers:
        - name: main
          image: ubuntu:latest
          env:
            - name: IDS_CSV_FILE
              value: /tmp/ids.csv
            - name: CRAWL_ORGS
              value: pingcap tikv tidbcloud PingCAP-QE
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
            - name: CACHE_BUCKET
              value: prow-job-cache
          volumeMounts:
            - name: insight-config
              mountPath: /tmp/ids.csv
              subPath: pingcap-ids.csv
          command:
            - /bin/bash
            - -ceo
            - pipefail
          args:
            - |
              # Install dependencies
              apt-get update && apt-get install -y git curl wget jq

              # Set up pixi.
              curl -fsSL https://pixi.sh/install.sh | bash
              export PATH="$HOME/.pixi/bin:$PATH"
              pixi install
              # install some packages with pixi

              # Prepare for crawl
              mkdir -p ../data/repos
              cp "$IDS_CSV_FILE" ../data/.ci/config/pingcap-ids.csv

              # Crawl.
              for org in $CRAWL_ORGS; do
                # Try to restore the git clone folders from cache
                # CACHE_FILE="repos-${org}.tar.gz"
                # CACHE_PATH="gs://${CACHE_BUCKET}/pingcap-qe/ci/${JOB_NAME}/${CACHE_FILE}"
                # echo "⏬ Trying to restore cache from ${CACHE_PATH} ..."
                # if gcloud storage objects describe ${CACHE_PATH} --quiet 2>/dev/null; then
                #   echo "Cache found! Downloading and extracting..."
                #   gcloud storage cp ${CACHE_PATH} /tmp/${CACHE_FILE}
                #   tar -xzf /tmp/${CACHE_FILE} -C ../data/repos $org
                #   rm /tmp/${CACHE_FILE}
                #   echo "Cache restored successfully!"
                # else
                #   echo "No cache found cloned repos, will crawl from scratch."
                # fi

                # Crawl commits in 11 years
                pixi run ../data/.ci/crawl_org_commits.sh $org ../data ../data/.ci/config 132

                # Save cache for ../data/repos/${org}
                # tar -czf /tmp/${CACHE_FILE} -C ../data/repos $org
                # echo "Uploading cache to ${CACHE_PATH}..."
                # gcloud storage cp /tmp/${CACHE_FILE} ${CACHE_PATH}
                # rm /tmp/${CACHE_FILE}
                # echo "Cache saved successfully!"
              done

              # Cleanup
              rm ../data/.ci/config/pingcap-ids.csv

              # Publish data.
              pushd ../data
                # Set git account
                git config user.name "ti-chi-bot[bot]"
                git config user.email "108142056+ti-chi-bot[bot]@users.noreply.github.com"
                git remote add origin https://${GITHUB_TOKEN}@github.com/PingCAP-QE/gitinsight-data.git

                # Checkout to a new branch with timestamp
                head_branch="auto/update-data-commits-$(date +%Y%m%d%H%M%S)"
                pr_title="Update commits data at $(date +%Y-%m-%d)"
                pr_desc="It is made by periodic jobs for data update. [skip ci]"

                git checkout -b $head_branch
                git add github

                git commit -m "${pr_title} [skip ci]"
                git push --set-upstream origin "$head_branch"

                # create a pull request and merge it.
                which gh || pixi global install gh
                gh pr create --title "$pr_title" --body "$pr_desc" --head $head_branch --base main
                gh pr merge --auto --squash --delete-branch || echo "Some error happened when merge the pr"
              popd
          resources:
            requests:
              cpu: "2"
              memory: 2Gi
            limits:
              cpu: "8"
              memory: 8Gi
      volumes:
        - name: insight-config
          secret:
            secretName: job-insight
  - name: periodic-crawl-pulls-issues
    cron: "30 * * * *" # @hourly
    hidden: true
    decorate: true
    decoration_config:
      timeout: 23h30m
      oauth_token_secret:
        name: github-token
        key: token
    extra_refs:
      - org: PingCAP-QE
        repo: gitinsight
        path_alias: app
        base_ref: main
        clone_depth: 1
        workdir: true
      - org: PingCAP-QE
        repo: gitinsight-data
        base_ref: main
        path_alias: data
        workdir: false
    max_concurrency: 1 # Only allow one instance of this job to run at a time
    spec:
      containers:
        - name: main
          image: ubuntu:latest
          env:
            - name: CRAWL_REPOS
              value: pingcap/tidb pingcap/tiflash pingcap/tiflow tikv/tikv tikv/pd
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_TOKEN
            - name: GITHUB_USER
              valueFrom:
                secretKeyRef:
                  name: job-insight
                  key: GITHUB_USER
          command:
            - /bin/bash
            - -ceo
            - pipefail
          args:
            - |
              # Install dependencies
              apt-get update && apt-get install -y git curl wget jq

              # Set up pixi.
              curl -fsSL https://pixi.sh/install.sh | bash
              export PATH="$HOME/.pixi/bin:$PATH"
              pixi install
              # install some packages with pixi

              # Crawl.
              for repo in $CRAWL_REPOS; do
                # Crawl pull and issues since 2015
                timeout 82800 pixi run ../data/.ci/crawl_repo_pulls_issues.sh $repo ../data 2015-01-01 || echo "⌚️ timeout in 23 hour."
              done

              # Publish data.
              pushd ../data
                # Set git account
                git config user.name "ti-chi-bot[bot]"
                git config user.email "108142056+ti-chi-bot[bot]@users.noreply.github.com"
                git remote add origin https://${GITHUB_TOKEN}@github.com/PingCAP-QE/gitinsight-data.git

                # Checkout to a new branch with timestamp
                head_branch="auto/update-data-issues-pulls-$(date +%Y%m%d%H%M%S)"
                pr_title="Update issues and pulls data at $(date +%Y-%m-%d)"
                pr_desc="It is made by periodic jobs for data update. [skip ci]"

                git checkout -b $head_branch
                git add github

                git commit -m "${pr_title} [skip ci]"
                git push --set-upstream origin $head_branch

                # create a pull request and merge it.
                which gh || pixi global install gh
                gh pr create --title "$pr_title" --body "$pr_desc" --head $head_branch --base main
                gh pr merge --auto --squash --delete-branch || echo "Some error happened when merge the pr"
              popd
          resources:
            requests:
              cpu: "1"
              memory: 1Gi
            limits:
              cpu: "4"
              memory: 4Gi
      volumes:
        - name: insight-config
          secret:
            secretName: job-insight
