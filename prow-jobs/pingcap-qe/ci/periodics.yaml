# struct ref: https://pkg.go.dev/sigs.k8s.io/prow/pkg/config#Periodic
periodics:
  - name: periodic-crawl-ci-run-data
    decorate: true # need add this.
    cron: "0 * * * *" # @hourly
    hidden: true
    spec:
      activeDeadlineSeconds: 300
      containers:
        - name: main
          image: denoland/deno:2.2.3
          command:
            - deno
            - run
            - --allow-net
            - https://github.com/PingCAP-QE/ee-apps/raw/main/insight/crawlers/ci/prow-jobs.ts
          args: [--dsn=$(DSN)]
          env:
            - name: DSN
              valueFrom:
                secretKeyRef:
                  key: DSN
                  name: job-insight
          resources:
            requests:
              memory: 1Gi
              cpu: 200m
  - name: periodic-reporter-flaky-tests-tidb
    decorate: true # need add this.
    cron: "0 1 * * 1" # Run weekly at 9:00 UTC+8 (which is 1:00 UTC) every Monday
    hidden: true
    spec:
      activeDeadlineSeconds: 300
      containers:
        - name: main
          image: denoland/deno:2.5.2
          command: [base, -c]
          args:
            - |
              #!/usr/bin/env bash
              set -euxo pipefail

              script_url="https://github.com/PingCAP-QE/ci/raw/main/tools/reporters/ci/flaky-tests/main.ts"

              start_date=$(date -v-7d +%Y-%m-%d)
              end_date=$(date +%Y-%m-%d)
              email_subject="Flaky Tests Report of pingcap/tidb repository between ${start_date} and ${end_date}"

              deno run --allow-env --allow-net --allow-write ${script_url} \
                --repo pingcap/tidb \
                --branch master \
                --from ${start_date} \
                --to ${end_date} \
                --threshold-ms 100000 \
                --db-url ${DSN} \
                --email-to $EMAIL_TO \
                --email-cc $EMAIL_CC \
                --email-from $SMTP_USER \
                --email-subject "${email_subject}"
          env:
            - name: SMTP_SECURE
              value: "true"
            - name: SMTP_HOST
              valueFrom:
                secretKeyRef:
                  key: smtp-host
                  name: CI_MAIL
            - name: SMTP_PORT
              valueFrom:
                secretKeyRef:
                  key: smtp-port
                  name: CI_MAIL
            - name: SMTP_USER
              valueFrom:
                secretKeyRef:
                  key: smtp-user
                  name: CI_MAIL
            - name: SMTP_PASS
              valueFrom:
                secretKeyRef:
                  key: smtp-password
                  name: CI_MAIL
            - name: DSN
              valueFrom:
                secretKeyRef:
                  key: DSN
                  name: insight-db-readonly
          resources:
            requests:
              memory: 1Gi
              cpu: 200m

prow_ignored: # currently we setup self-hosted github action runners to run for the jobs.
  periodics:
    - name: periodic-crawl-commits
      cron: "0 * * * *" # Run @hourly
      hidden: true
      decorate: true
      decoration_config:
        timeout: 12h
        oauth_token_secret:
          name: github-token
          key: token
      extra_refs:
        - org: PingCAP-QE
          repo: gitinsight
          path_alias: app
          base_ref: main
          clone_depth: 1
          workdir: true
        - org: PingCAP-QE
          repo: gitinsight-data
          base_ref: main
          path_alias: data
          workdir: false
      max_concurrency: 1 # Only allow one instance of this job to run at a time
      spec:
        containers:
          - name: main
            image: ubuntu:latest
            env:
              - name: CRAWL_ORGS
                value: pingcap tikv tidbcloud PingCAP-QE
              - name: GITHUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: job-insight
                    key: GITHUB_TOKEN
              - name: GITHUB_USER
                valueFrom:
                  secretKeyRef:
                    name: job-insight
                    key: GITHUB_USER
              - name: CACHE_BUCKET
                value: prow-job-cache
            volumeMounts:
              - name: insight-config # has keys: pingcap-ids.csv
                mountPath: /tmp/config
            command:
              - /bin/bash
              - -ceo
              - pipefail
            args:
              - |
                # Install dependencies
                apt-get update && apt-get install -y git curl wget jq

                # Set up pixi.
                curl -fsSL https://pixi.sh/install.sh | bash
                export PATH="$HOME/.pixi/bin:$PATH"
                pixi install
                # install some packages with pixi

                # Prepare for crawl
                mkdir -p ../data/repos
                cp -rv /tmp/config/* ../data/.ci/config/

                # Crawl.
                for org in $CRAWL_ORGS; do
                  # Try to restore the git clone folders from cache
                  # CACHE_FILE="repos-${org}.tar.gz"
                  # CACHE_PATH="gs://${CACHE_BUCKET}/pingcap-qe/ci/${JOB_NAME}/${CACHE_FILE}"
                  # echo "⏬ Trying to restore cache from ${CACHE_PATH} ..."
                  # if gcloud storage objects describe ${CACHE_PATH} --quiet 2>/dev/null; then
                  #   echo "Cache found! Downloading and extracting..."
                  #   gcloud storage cp ${CACHE_PATH} /tmp/${CACHE_FILE}
                  #   tar -xzf /tmp/${CACHE_FILE} -C ../data/repos $org
                  #   rm /tmp/${CACHE_FILE}
                  #   echo "Cache restored successfully!"
                  # else
                  #   echo "No cache found cloned repos, will crawl from scratch."
                  # fi

                  # Crawl commits in 11 years
                  ls -l ../data/.ci/config/pingcap-ids.csv
                  pixi run ../data/.ci/crawl_org_commits.sh $org ../data ../data/.ci/config 132

                  # Save cache for ../data/repos/${org}
                  # tar -czf /tmp/${CACHE_FILE} -C ../data/repos $org
                  # echo "Uploading cache to ${CACHE_PATH}..."
                  # gcloud storage cp /tmp/${CACHE_FILE} ${CACHE_PATH}
                  # rm /tmp/${CACHE_FILE}
                  # echo "Cache saved successfully!"
                done

                # Publish data.
                pushd ../data
                  # Cleanup
                  # Discard untracked files in .ci folder
                  git clean -fd .ci

                  # Set git account
                  git config user.name "ti-chi-bot[bot]"
                  git config user.email "108142056+ti-chi-bot[bot]@users.noreply.github.com"
                  git remote add origin https://${GITHUB_TOKEN}@github.com/PingCAP-QE/gitinsight-data.git

                  # Checkout to a new branch with timestamp
                  head_branch="auto/update-data-commits-$(date +%Y%m%d%H%M%S)"
                  pr_title="Update commits data at $(date +%Y-%m-%d)"
                  pr_desc="It is made by periodic jobs for data update. [skip ci]"

                  git checkout -b $head_branch
                  git add github

                  git commit -m "${pr_title} [skip ci]"
                  git push --set-upstream origin "$head_branch"

                  # create a pull request and merge it.
                  which gh || pixi global install gh
                  gh pr create --title "$pr_title" --body "$pr_desc" --head $head_branch --base main
                  gh pr merge --auto --squash --delete-branch || echo "Some error happened when merge the pr"
                popd
            resources:
              requests:
                cpu: "2"
                memory: 2Gi
              limits:
                cpu: "8"
                memory: 8Gi
        volumes:
          - name: insight-config
            secret:
              secretName: job-insight
    - name: periodic-crawl-pulls-issues
      cron: "30 * * * *" # @hourly
      hidden: true
      decorate: true
      decoration_config:
        timeout: 23h30m
        oauth_token_secret:
          name: github-token
          key: token
      extra_refs:
        - org: PingCAP-QE
          repo: gitinsight
          path_alias: app
          base_ref: main
          clone_depth: 1
          workdir: true
        - org: PingCAP-QE
          repo: gitinsight-data
          base_ref: main
          path_alias: data
          workdir: false
      max_concurrency: 1 # Only allow one instance of this job to run at a time
      spec:
        containers:
          - name: main
            image: ubuntu:latest
            env:
              - name: CRAWL_REPOS
                value: pingcap/tidb pingcap/tiflash pingcap/tiflow tikv/tikv tikv/pd
              - name: GITHUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: job-insight
                    key: GITHUB_TOKEN
              - name: GITHUB_USER
                valueFrom:
                  secretKeyRef:
                    name: job-insight
                    key: GITHUB_USER
            command:
              - /bin/bash
              - -ceo
              - pipefail
            args:
              - |
                # Install dependencies
                apt-get update && apt-get install -y git curl wget jq

                # Set up pixi.
                curl -fsSL https://pixi.sh/install.sh | bash
                export PATH="$HOME/.pixi/bin:$PATH"
                pixi install
                # install some packages with pixi

                # Crawl.
                for repo in $CRAWL_REPOS; do
                  # Crawl pull and issues since 2015
                  timeout 82800 pixi run ../data/.ci/crawl_repo_pulls_issues.sh $repo ../data 2015-01-01 || echo "⌚️ timeout in 23 hour."
                done

                # Publish data.
                pushd ../data
                  # Set git account
                  git config user.name "ti-chi-bot[bot]"
                  git config user.email "108142056+ti-chi-bot[bot]@users.noreply.github.com"
                  git remote add origin https://${GITHUB_TOKEN}@github.com/PingCAP-QE/gitinsight-data.git

                  # Checkout to a new branch with timestamp
                  head_branch="auto/update-data-issues-pulls-$(date +%Y%m%d%H%M%S)"
                  pr_title="Update issues and pulls data at $(date +%Y-%m-%d)"
                  pr_desc="It is made by periodic jobs for data update. [skip ci]"

                  git checkout -b $head_branch
                  git add github

                  git commit -m "${pr_title} [skip ci]"
                  git push --set-upstream origin $head_branch

                  # create a pull request and merge it.
                  which gh || pixi global install gh
                  gh pr create --title "$pr_title" --body "$pr_desc" --head $head_branch --base main
                  gh pr merge --auto --squash --delete-branch || echo "Some error happened when merge the pr"
                popd
            resources:
              requests:
                cpu: "4"
                memory: 4Gi
              limits:
                cpu: "8"
                memory: 8Gi
        volumes:
          - name: insight-config
            secret:
              secretName: job-insight
